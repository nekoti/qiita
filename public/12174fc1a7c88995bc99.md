---
title: Hadooopを疑似分散モードで動かしてみます。 (CentOS6 + Hadoop + Hive)
tags:
  - hadoop
  - hive
private: false
updated_at: '2018-09-23T16:53:06+09:00'
id: 12174fc1a7c88995bc99
organization_url_name: null
slide: false
ignorePublish: false
---
Hadoopの勉強のため色々なサイトから情報を集めて実験環境を構築した時のメモです。
なおセキュリティの考慮は一切しておりません。

# 実験環境
| 名称 | バージョン |
|:-----------:|:------------:|
|CentOS|6.10|
|openjdk|1.8.0|
|Hadoop|2.9.1|
|Derby|10.13.1.1|
|Hive|2.3.3|

# 環境の確認
## バージョン確認
### OSのバージョン確認
- コマンド<br>

```
$ cat /etc/redhat-release
CentOS release 6.10 (Final)
```

### opensslのバージョン確認

```
$ openssl version
OpenSSL 1.0.1e-fips 11 Feb 2013
```

### openjdkのバージョン確認

```
$ java -version
openjdk version "1.8.0_181"
OpenJDK Runtime Environment (build 1.8.0_181-b13)
OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)
```

# 事前準備
## 作業用ユーザーの作成
これからインストールするものは作業用ユーザーhadoop権限で実行させる。<br>

- コマンド

```
# useradd -m /home/hadoop -m -g users -s /bin/bash hadoop
# passwd hadoop
Changing password for user hadoop.
New password:
Retype new password:
passwd: all authentication tokens updated successfully.
```

## ユーザー環境変数設定（１）

- 作業用ユーザーに切り替える。<br>

```
# su - hadoop
# vi .bsash_profile
```
- 作業用ユーザーの環境変数を設定する。<br>

``` ~/.bash_profileの追加設定
# Settings for Java
export JAVA_HOME=/usr/lib/jvm/java-1.8.0
export PATH=$JAVA_HOME/bin${PATH:+:$PATH}
```

- 社内のネットワーク等でプロキシサーバーの設定が必要であれば追加しておく。<br>

``` ~/.bash_profileの追加設定（例）
export http_proxy=http://PROXY_SERVER_NAME:PORT/
export https_proxy=http://PROXY_SERVER_NAME:PORT/
export no_proxy=localhost 127.0.0.1/8
```

- 環境変数を再読みする。<br>

```
 . ~/.bash_profile
```

# Hadoop
## インストール

- rootユーザーに切り替える。<br>

```
$ su - root
```

- バイナリファイルをダウンロードする。<br>

```
$ su - root
# cd /tmp
# wget http://ftp.riken.jp/net/apache/hadoop/common/hadoop-2.9.1/hadoop-2.9.1.tar.gz
```

- アーカイブファイルを展開する。<br>

```
# tar xvzf hadoop-2.9.1.tar.gz
```

- 展開したディレクトリを移動する。<br>

```
# mv hadoop-2.9.1 /usr/local
```

- シンボリックリンクを作成する。<br>

```
# cd /usr/local
# ln -s hadoop-2.9.1 hadoop
```

- パーミッションを設定する。<br>

```
# chown -R hadoop.users hadoop-2.9.1/
# chown -R hadoop.users ./hadoop
```

### ユーザー環境変数設定（２）

- 作業用ユーザーに切り替える。<br>

```
# su - hadoop
$ vi .bsash_profile
```

- 作業用ユーザーの環境変数を設定する。<br>

``` ~/.bash_profileの追加設定
# Settings for Hadoop
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_COMMON_LIB_NATIVE_DIR=$HAOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin${PATH:+:$PATH}
```

- 環境変数再読み<br>

```
$ . ~/.bash_profile
```


### Hadoop動作確認

- バージョン確認<br>

```
$ hadoop version
```

- 出力例<br>

```
Hadoop 2.9.1
Subversion https://github.com/apache/hadoop.git -r e30710aea4e6e55e69372929106cf119af06fd0e
Compiled by root on 2018-04-16T09:33Z
Compiled with protoc 2.5.0
From source with checksum 7d6d2b655115c6cc336d662cc2b919bd
This command was run using /usr/local/hadoop-2.9.1/share/hadoop/common/hadoop-common-2.9.1.jar
```

- サンプルプログラム起動（円周率）<br>

```
$ hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar pi 10 1000
```

- 出力例<br>

```
Number of Maps  = 10
Samples per Map = 1000
YY/MM/DD HH:MM:SS WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Wrote input for Map #0
Wrote input for Map #1
Wrote input for Map #2
Wrote input for Map #3
Wrote input for Map #4
Wrote input for Map #5
Wrote input for Map #6
Wrote input for Map #7
Wrote input for Map #8
Wrote input for Map #9
Starting Job
YY/MM/DD HH:MM:SS INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id

～中略～

        Shuffle Errors
                BAD_ID=0
                CONNECTION=0
                IO_ERROR=0
                WRONG_LENGTH=0
                WRONG_MAP=0
                WRONG_REDUCE=0
        File Input Format Counters
                Bytes Read=1300
        File Output Format Counters
                Bytes Written=109
Job Finished in 4.201 seconds
Estimated value of Pi is 3.14080000000000000000
```

## 疑似分散モードの設定
### SSHの設定

- 作業用ユーザーに切り替える。<br>

```
# su - hadoop
```

- SSHの設定<br>
パスワードなしでログインできるように公開鍵のペアを作成する。<br>

- コマンド<br>

```
$ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
```

- 出力例<br>

```
Generating public/private rsa key pair.
Created directory '/home/hadoop/.ssh'.
Your identification has been saved in /home/hadoop/.ssh/id_rsa.
Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.
The key fingerprint is:
83:29:72:28:7e:d9:5c:04:1c:91:1d:73:bd:4e:f6:ec worker@centos6
The key's randomart image is:
+--[ RSA 2048]----+
|    .+=o...      |
|     o..o  .     |
|       .    .    |
|   .  .o   +     |
|. o o o.S + o    |
|.. o+..  . . o   |
| . o o      .    |
|  .          E   |
|                 |
+-----------------+
```

- コマンド<br>

```
$ ssh-copy-id -i ~/.ssh/id_rsa.pub localhost
```

- 出力例<br>

```
The authenticity of host 'localhost (::1)' can't be established.
RSA key fingerprint is 27:6f:10:df:a8:ef:c5:5a:f6:c5:19:36:2d:73:24:a1.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'localhost' (RSA) to the list of known hosts.
hadoop@localhost's password:
Now try logging into the machine, with "ssh 'localhost'", and check in:

  .ssh/authorized_keys

to make sure we haven't added extra keys that you weren't expecting.
```

- その他<br>
SSHのポート番号が22でない場合、~/.ssh/configに指定すれば良いらしい。 <br>

```
Host example.com 
  Port 12345 
```

### Hadoopの設定
#### core-site.xml

- 設定ファイルを編集する。<br>

|ファイル名|
|:-----------|
|/usr/local/hadoop/etc/hadoop/core-site.xml|

- 変更前<br>

```
<!-- Put site-specific property overrides in this file. -->

<configuration>
</configuration>
```

- 変更前<br>

```
<!-- Put site-specific property overrides in this file. -->

<configuration>

</configuration>
```

- 変更後<br>

```
<!-- Put site-specific property overrides in this file. -->

<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://loclhost:9000</value>
  </property>
</configuration>
```

#### hdfs-site.xml

- 設定ファイルを編集する。<br>

|ファイル名|
|:-----------|
|/usr/local/hadoop/etc/hadoop/hdfs-site.xml|

- 変更前<br>

```
<!-- Put site-specific property overrides in this file. -->

<configuration>

</configuration>
```

- 変更後<br>
シングルホストで動作させるので、replicationの値は1にする。<br>

```
<!-- Put site-specific property overrides in this file. -->

<configuration>
  <property>
    <name>hdfs.replication</name>
    <value>1</value>
  </property>
</configuration>
```

#### yarn-site.xml
- 設定ファイルを編集する。<br>

|ファイル名|
|:-----------|
|/usr/local/hadoop/etc/hadoop/yarn-site.xml

- 変更前<br>

```
<configuration>

<!-- Site specific YARN configuration properties -->

</configuration>
```

- 変更後<br>

```
<configuration>

<!-- Site specific YARN configuration properties -->
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>

  <property>
    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>

</configuration>
```

#### hadoop-env.sh

- 設定ファイルを編集する。<br>

|ファイル名|
|:-----------|
|/usr/local/hadoop/etc/hadoop/hadoop-env.sh|

SSHポート番号が22と異なる場合は HADOOP_SSH_OPTS="-p 1234" のように指定しておく。<br>

- 変更前<br>

```
# The java implementation to use.
export JAVA_HOME=${JAVA_HOME}
```

- 変更後<br>

```
# The java implementation to use.
#export JAVA_HOME=${JAVA_HOME}
export JAVA_HOME=/usr/lib/jvm/java-1.8.0
```

### HDFS初期化

- コマンド<br>

```
$ hdfs namenode -format
```

- 出力例<br>

```
YY/MM/DD HH:MM:SS INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = localhost/127.0.0.1
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 2.9.1

～中略～

YY/MM/DD HH:MM:SS INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at localhost/127.0.0.1
************************************************************/
```

### サービス起動・停止
#### サービス起動

- コマンド<br>

```
$ start-dfs.sh; start-yarn.sh
```

- 起動確認<br>

```
$ jps
```

- 出力例<br>

```
5858 ResourceManager
5397 NameNode
6121 Jps
5706 SecondaryNameNode
5534 DataNode
5967 NodeManager
```


#### サービス停止

- コマンド<br>

```
$ stop-yarn.sh; stop-dfs.sh
```

# Derby
## インストール

- rootユーザーに切り替える。<br>

```
$ su - root
```

- バイナリファイルをダウンロードする。<br>

```
$ su - root
# cd /tmp
# wget http://ftp.riken.jp/net/apache/db/derby/db-derby-10.13.1.1/db-derby-10.13.1.1-bin.tar.gz
```

- アーカイブファイルを展開する。<br>

```
# tar xvzf db-derby-10.13.1.1-bin.tar.gz
```

- 展開したディレクトリを移動する。<br>

```
# mv db-derby-10.13.1.1-bin /usr/local
```

- シンボリックリンクを作成する。<br>

```
# cd /usr/local
# ln -s db-derby-10.13.1.1-bin/ db-derby
```

### ユーザー環境変数設定（３）

- 作業用ユーザーに切り替える。<br>

```
# su - hadoop
$ vi .bsash_profile
```

- 作業用ユーザーの環境変数を設定する。<br>

``` ~/.bash_profileの追加設定
# Settings for Derby
export DERBY_HOME=/usr/local/db-derby
export PATH=$DERBY_HOME/bin${PATH:+:$PATH}
export CLASSPATH=$CLASSPATH:$DERBY_HOME/lib/derby.jar:$DERBY_HOME/lib/derbytools.jar
```

- 環境変数再読み<br>

```
$ . ~/.bash_profile
```

# Hive
## インストール

- rootユーザーに切り替える。<br>

```
$ su - root
```

- バイナリファイルをダウンロードする。<br>

```
$ su - root
# cd /tmp
# wget http://ftp.jaist.ac.jp/pub/apache/hive/hive-2.3.3/apache-hive-2.3.3-bin.tar.gz
```

- アーカイブファイルを展開する。<br>

```
# tar xvzf apache-hive-2.3.3-bin.tar.gz
```

- 展開したディレクトリを移動する。<br>

```
# mv apache-hive-2.3.3-bin /usr/local
```

- シンボリックリンクを作成する。<br>

```
# cd /usr/local
# ln -s apache-hive-2.3.3-bin hive
```

- パーミッションを設定する。<br>

```
# chown -R hadoop.users apache-hive-2.3.3-bin/
# chown -R hadoop.users ./hive
```

### ユーザー環境変数設定（４）

- 作業用ユーザーに切り替える。<br>

```
# su - hadoop
$ vi .bsash_profile
```

- 作業用ユーザーの環境変数を設定する。<br>

``` ~/.bash_profileの追加設定
# Settings for Hive
export HIVE_HOME=/usr/local/hive
export PATH=$HIVE_HOME/bin${PATH:+:$PATH}
```

- 環境変数再読み<br>

```
$ . ~/.bash_profile
```

### 一時ディレクトリ作成

- 作業用ユーザーに切り替える。<br>

```
# su - hadoop
```

- 一時ディレクトリ作成<br>

```
$ mkdir /usr/local/hive/tmp
$ chown -R hadoop.users /usr/local/hive/tmp
```

### Hiveの設定
#### hive-site.xml

- テンプレートファイルをコピーする。<br>

```
$ cd /usr/local/hive
$ cp -p hive-default.xml.template hive-site.xml
```

- 設定ファイルを編集する。<br>

|ファイル名|
|:-----------|
|/usr/local/hive/conf/hive-site.xml|

- 変更前１<br>

```
--><configuration>
  <!-- WARNING!!! This file is auto generated for documentation purposes ONLY! -->
  <!-- WARNING!!! Any changes you make to this file will be ignored by Hive.   -->
  <!-- WARNING!!! You must make your changes in hive-site.xml instead.         -->
  <!-- Hive Execution Parameters -->
  <property>
    <name>hive.exec.script.wrapper</name>
    <value/>
    <description/>
  </property>
```

- 変更後１<br>

```
--><configuration>
  <!-- WARNING!!! This file is auto generated for documentation purposes ONLY! -->
  <!-- WARNING!!! Any changes you make to this file will be ignored by Hive.   -->
  <!-- WARNING!!! You must make your changes in hive-site.xml instead.         -->
  <!-- Hive Execution Parameters -->
  <property>
    <name>system:java.io.tmpdir</name>
    <value>/usr/local/hive/tmp</value>
    <description/>
  </property>
  <property>
    <name>system:user.name</name>
    <value>${user.name}</value>
    <description/>
  </property>
  <property>
    <name>hive.exec.script.wrapper</name>
    <value/>
    <description/>
  </property>
```

- 変更前２<br>

```
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:derby:;databaseName=metastore_db;create=true</value>
    <description>
      JDBC connect string for a JDBC metastore.
      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
    </description>
  </property>
```

- 変更後２<br>

```
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <!--<value>jdbc:derby:;databaseName=metastore_db;create=true</value>-->
    <value>jdbc:derby:;databaseName=/home/hadoop/metastore_db;create=true</value>
    <description>
      JDBC connect string for a JDBC metastore.
      To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL.
      For example, jdbc:postgresql://myhost/db?ssl=true for postgres database.
    </description>
  </property>
```

### JDBC Metastore初期化

- 作業用ユーザーに切り替える。<br>

```
# su - hadoop
# cd ~/
```

- Hive上に作成されるテーブル情報を格納するためのJDBC Metastoreの作成・初期化する。<br>

```
$ schematool -dbType derby -initSchema
```


